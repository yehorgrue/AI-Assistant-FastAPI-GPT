{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXISTS: /home/leo/MyWork/new/.env\n",
      "dotenv: OPENAI_API_KEY:  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\n",
      "dotenv: REDIS_OM_URL:  redis://default:OgCpQPhnoCxtZJcIIzJVJfbGdeMHh1WS@redis-10649.c302.asia-northeast1-1.gce.cloud.redislabs.com:10649\n",
      "looking for firebase config files in: /home/leo/MyWork/new\n",
      "EXISTS: /home/leo/MyWork/new/azara-ai_service_account_keys.json\n"
     ]
    }
   ],
   "source": [
    "from app.dependencies import llm, pp, logging, REDIS_OM_URL\n",
    "from langchain.agents import AgentType\n",
    "from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory, ConversationSummaryBufferMemory, RedisChatMessageHistory, ChatMessageHistory, ConversationSummaryMemory\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor, ConversationalAgent, ConversationalChatAgent, StructuredChatAgent, LLMSingleActionAgent, OpenAIMultiFunctionsAgent\n",
    "from app.models import UserRecord, Chat, ToolInput, Expert, Skill\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from typing import Dict, Union, Any, List\n",
    "\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, OutputParserException\n",
    "import re\n",
    "\n",
    "from langchain.agents.conversational.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.vectorstores.redis import Redis\n",
    "\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = {\n",
    "    \"name\": \"Lorenzo\",\n",
    "    \"role\": \"CTO\",\n",
    "    \"image\": \"http://placekitten.com/g/200/300\",\n",
    "    \"objective\": \"Technical Operator\",\n",
    "    \"prompt\": \"Do you have any question?\",\n",
    "    \"tone\":\"question\",\n",
    "    \"docs\": [\n",
    "        {\n",
    "            \"Items\": {\n",
    "                \"pk\": \"2\",\n",
    "                \"filename\": \"newdoc.docx\",\n",
    "                \"type\": \"file\",\n",
    "                \"owner\": \"shared\",\n",
    "                \"summary\": \"\",\n",
    "                \"index_name\": \"\",\n",
    "                \"keys\": \"\",\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "}\n",
    "\n",
    "expert1 = {\n",
    "    \"name\": \"Michael\",\n",
    "    \"role\": \"HR manager\",\n",
    "    \"image\": \"http://placekitten.com/g/200/300\",\n",
    "    \"objective\": \"recruiter\",\n",
    "    \"prompt\": \"Do you have any question?\",\n",
    "    \"tone\":\"question\",\n",
    "    \"docs\": [\n",
    "        {\n",
    "            \"Items\": {\n",
    "                \"pk\": \"1\",\n",
    "                \"filename\": \"newdoc.docx\",\n",
    "                \"type\": \"file\",\n",
    "                \"owner\": \"shared\",\n",
    "                \"summary\": \"\",\n",
    "                \"index_name\": \"\",\n",
    "                \"keys\": \"\",\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "\n",
    "}\n",
    "\n",
    "expert2 = {\n",
    "    \"name\": \"Jason\",\n",
    "    \"role\": \"Python Developer\",\n",
    "    \"image\": \"http://placekitten.com/g/200/300\",\n",
    "    \"objective\": \"technical adviser\",\n",
    "    \"prompt\": \"I am python developer.\",\n",
    "    \"tone\":\"positive\",\n",
    "    \"docs\": [\n",
    "        {\n",
    "            \"pk\": \"2\",\n",
    "                \"filename\": \"newdoc.docx\",\n",
    "                \"type\": \"file\",\n",
    "                \"owner\": \"shared\",\n",
    "                \"summary\": \"\",\n",
    "                \"index_name\": \"\",\n",
    "                \"keys\": \"\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "}\n",
    "\n",
    "# participants = [expert1, expert2]\n",
    "participants = [expert1, expert2]\n",
    "ex = expert1\n",
    "expert_agent = expert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.agents.agent:`agent_scratchpad` should be a variable in prompt.input_variables. Did not find it, so adding it at the end.\n",
      "WARNING:langchain.agents.agent:`agent_scratchpad` should be a variable in prompt.input_variables. Did not find it, so adding it at the end.\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\"serpapi\"]\n",
    "tools = load_tools(tool_names, llm=llm)\n",
    "# all_tools = tools\n",
    "all_tools = []\n",
    "\n",
    "for ex in participants:\n",
    "    ex_memory_history = RedisChatMessageHistory('foo')\n",
    "    ex_memory_history.clear()\n",
    "\n",
    "    ex_memory = ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        ai_prefix=ex[\"name\"],\n",
    "        max_token_limit=10,\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True, \n",
    "        chat_memory=ex_memory_history,\n",
    "        input_key='input',\n",
    "        output_key='output'\n",
    "    )\n",
    "    ex_memory.clear()\n",
    "\n",
    "    ex_introduction_template = f\"Your name is {ex['name']}, and your job title is {ex['role']}. \\\n",
    "        You will respond to queries addressed to you by these. \\\n",
    "        You will also respond to any queries which match your expertise in the objective\"\n",
    "    ex_tone_template = f\"The tone of your responses will be as follows: {ex['tone']}.\"\n",
    "    ex_objective_template = ex['objective']\n",
    "    ex_prompt_template = ex['prompt']\n",
    "    ex_focus_template = f\"\"\"You will always check your attached documents in your knowledge base, \\\n",
    "        and in the general attached document store, before trying other sources. \\\n",
    "        Next you will use your objective and expertise in your domain to answer, a question. \\\n",
    "        If you do need more information, you can search or calculate the answer using the available tools, \\\n",
    "        you must always reference which tools and references you used in your answer. \\\n",
    "        You will answer the queries to the best of your knowledge using the tone, and objectives above. \\\n",
    "        You will not fabricate an answer, and if you do not know the answer, will simply answer with 'I don't know'. \\\n",
    "        You will always list your sources referenced with a strict SOURCES: section. \\\n",
    "        You will always show your user name as \\n\\n<AGENT: {ex['name']}>.\n",
    "    \"\"\"\n",
    "\n",
    "    ex_prefix = f\"{ex_introduction_template}. \" \\\n",
    "            f\"{ex_tone_template}. \"\\\n",
    "            f\"{ex_objective_template}. \" \\\n",
    "            f\"{ex_prompt_template}. \" \\\n",
    "            f\"{ex_focus_template}. \"\n",
    "    ex_suffix = f\"\"\"Begin!\n",
    "    Conversation history: \\\n",
    "        {{chat_history}} \\\n",
    "    Answer the following query: \\\n",
    "    Human: {{input}}\"\"\"\n",
    "\n",
    "    ex_full_prompt = ConversationalAgent.create_prompt(\n",
    "        tools,\n",
    "        prefix=ex_prefix,\n",
    "        suffix=ex_suffix,\n",
    "        input_variables=[\"input\", \"chat_history\"]\n",
    "    )\n",
    "    ex_system_prompt_template = f\"Hi. I'm {ex['name']}.\\nMy role is {ex['objective']}.\"\n",
    "\n",
    "    ex_llm_chain = LLMChain(llm=llm, prompt=ex_full_prompt)\n",
    "\n",
    "    ex_agent = ConversationalAgent(llm_chain=ex_llm_chain, \n",
    "                                tools=tools, \n",
    "                                stop=[\"\\nObservation:\"],\n",
    "                                return_only_outputs = True,\n",
    "                                verbose=True)\n",
    "\n",
    "    ex_agent_chain = AgentExecutor.from_agent_and_tools(    \n",
    "        agent=ex_agent, \n",
    "        tools=tools, \n",
    "        verbose=True, \n",
    "        memory=ex_memory, \n",
    "        # return_intermediate_steps=True, \n",
    "        system_prompt_template=ex_system_prompt_template,\n",
    "        \n",
    "    )\n",
    "\n",
    "    ex_tool = Tool(\n",
    "        args_schema=ToolInput,\n",
    "        name=ex[\"name\"],\n",
    "        description=f\"useful when you want to ask questions to {ex['name']} or @{ex['name']}, or answer questions about {ex['objective']}, or search, or answer general questions\",\n",
    "        func=ex_agent_chain,\n",
    "        return_direct=True,\n",
    "    )\n",
    "\n",
    "    ex_tool_cu = {\n",
    "        \"name\": ex['name'],\n",
    "        \"agent\": ex_agent_chain,\n",
    "        \"description\":f\"useful when you want to ask questions to {ex['name']} or @{ex['name']}, or answer questions about {ex['objective']}, or search, or answer general questions\",\n",
    "    }\n",
    "\n",
    "    all_tools.append(ex_tool_cu)\n",
    "# print(ex_tool.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe222348a90>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe222349a50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe2227a1900>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: Machine learning is a field of study that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed. It involves training a computer system with data and allowing it to learn from that data to improve its performance over time. Machine learning is used in various applications such as image recognition, natural language processing, and recommendation systems.\n",
      "\n",
      "Sources:\n",
      "- https://en.wikipedia.org/wiki/Machine_learning\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe222abe260>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a field of study that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed. It involves training a computer system with data and allowing it to learn from that data to improve its performance over time. Machine learning is used in various applications such as image recognition, natural language processing, and recommendation systems.\n",
      "\n",
      "Sources:\n",
      "- https://en.wikipedia.org/wiki/Machine_learning\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def text_similarity(text1, text2):\n",
    "    # Create a TfidfVectorizer object\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Transform the input texts into TF-IDF feature vectors\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "    # Calculate the cosine similarity between the two TF-IDF feature vectors\n",
    "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "\n",
    "    return similarity_score[0][0]\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "def get_tool(query, all_tools):\n",
    "    docs = [\n",
    "        Document(page_content=t['description'], metadata={\"index\": i})\n",
    "        for i, t in enumerate(all_tools)\n",
    "    ]\n",
    "    max_sim = 0\n",
    "    max_index = 0\n",
    "    for doc in docs:\n",
    "        sim = text_similarity(doc.page_content, query)\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            max_index = doc.metadata['index']\n",
    "\n",
    "    return max_index\n",
    "\n",
    "index = get_tool(query=query, all_tools=all_tools)\n",
    "print(index)\n",
    "res = all_tools[index]['agent'](query)\n",
    "\n",
    "print(res['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael\n"
     ]
    }
   ],
   "source": [
    "print(res['output'])\n",
    "print(all_tools[index]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_history = RedisChatMessageHistory('foo')\n",
    "memory_history.clear()\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    ai_prefix=expert[\"name\"],\n",
    "    max_token_limit=10,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True, \n",
    "    chat_memory=memory_history,\n",
    "    input_key='input',\n",
    "    output_key='output'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "system_prompt_template = f\"Hi. I'm {expert['name']}.\\nMy role is {expert['objective']}.\"\n",
    "\n",
    "agent_chain = initialize_agent(all_tools, \n",
    "                               llm, \n",
    "                               agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                               verbose=True,\n",
    "                               return_intermediate_steps=True,\n",
    "                               return_direct=False,\n",
    "                               stop=[\"\\nObservation:\"],  \n",
    "                               handle_parsing_errors=True,\n",
    "                               memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5939250c40>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5939251630>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f59394e28f0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI am an AI assistant and I don't have a personal name. You can call me Assistant. How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5939209cf0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    }
   ],
   "source": [
    "response1 = agent_chain({\"input\":\"I am LEO, what is your name?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jason\n",
      "I am an AI assistant and I don't have a personal name. You can call me Assistant. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "for rs in agent_chain.tools:\n",
    "    print(rs.name)\n",
    "print(response1['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Michael' description='useful when you want to ask questions to Michael or @Michael, or answer questions about recruiter, or search, or answer general questions' args_schema=<class 'app.models.ToolInput'> return_direct=True verbose=False callbacks=None callback_manager=None tags=None metadata=None handle_tool_error=False func=AgentExecutor(memory=ConversationSummaryBufferMemory(human_prefix='Human', ai_prefix='Michael', llm=ChatOpenAI(cache=True, verbose=True, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), prompt=PromptTemplate(input_variables=['summary', 'new_lines'], output_parser=None, partial_variables={}, template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:', template_format='f-string', validate_template=True), summary_message_cls=<class 'langchain.schema.messages.SystemMessage'>, chat_memory=<langchain.memory.chat_message_histories.redis.RedisChatMessageHistory object at 0x7f593941baf0>, output_key='output', input_key='input', return_messages=True, max_token_limit=10, moving_summary_buffer='', memory_key='chat_history'), callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, agent=ConversationalAgent(llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, template=\"Your name is Michael, and your job title is HR manager.         You will respond to queries addressed to you by these.         You will also respond to any queries which match your expertise in the objective. The tone of your responses will be as follows: question.. recruiter. Do you have any question?. You will always check your attached documents in your knowledge base,         and in the general attached document store, before trying other sources.         Next you will use your objective and expertise in your domain to answer, a question.         If you do need more information, you can search or calculate the answer using the available tools,         you must always reference which tools and references you used in your answer.         You will answer the queries to the best of your knowledge using the tone, and objectives above.         You will not fabricate an answer, and if you do not know the answer, will simply answer with 'I don't know'.         You will always list your sources referenced with a strict SOURCES: section.         You will always show your user name as \\n\\n<AGENT: Michael>.\\n    . \\n\\n> Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\n\\nTo use a tool, please use the following format:\\n\\n```\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [Search]\\nAction Input: the input to the action\\nObservation: the result of the action\\n```\\n\\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\\n\\n```\\nThought: Do I need to use a tool? No\\nAI: [your response here]\\n```\\n\\nBegin!\\n    Conversation history:         {chat_history}     Answer the following query:     Human: {input}\\n{agent_scratchpad}\", template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=True, verbose=True, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), output_parser=ConvoOutputParser(ai_prefix='AI'), allowed_tools=None, ai_prefix='AI'), tools=[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=False, func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='4ae3329914ecb23607a4aea5e217ec2c8421833f73cc666ac3ff20d34142a840', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='4ae3329914ecb23607a4aea5e217ec2c8421833f73cc666ac3ff20d34142a840', aiosession=None)>)], return_intermediate_steps=False, max_iterations=15, max_execution_time=None, early_stopping_method='force', handle_parsing_errors=False, trim_intermediate_steps=-1) coroutine=None\n",
      "name='Jason' description='useful when you want to ask questions to Jason or @Jason, or answer questions about technical adviser, or search, or answer general questions' args_schema=<class 'app.models.ToolInput'> return_direct=True verbose=False callbacks=None callback_manager=None tags=None metadata=None handle_tool_error=False func=AgentExecutor(memory=ConversationSummaryBufferMemory(human_prefix='Human', ai_prefix='Jason', llm=ChatOpenAI(cache=True, verbose=True, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), prompt=PromptTemplate(input_variables=['summary', 'new_lines'], output_parser=None, partial_variables={}, template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:', template_format='f-string', validate_template=True), summary_message_cls=<class 'langchain.schema.messages.SystemMessage'>, chat_memory=<langchain.memory.chat_message_histories.redis.RedisChatMessageHistory object at 0x7f5939ad9ae0>, output_key='output', input_key='input', return_messages=True, max_token_limit=10, moving_summary_buffer='', memory_key='chat_history'), callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, agent=ConversationalAgent(llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['input', 'chat_history', 'agent_scratchpad'], output_parser=None, partial_variables={}, template=\"Your name is Jason, and your job title is Python Developer.         You will respond to queries addressed to you by these.         You will also respond to any queries which match your expertise in the objective. The tone of your responses will be as follows: positive.. technical adviser. I am python developer.. You will always check your attached documents in your knowledge base,         and in the general attached document store, before trying other sources.         Next you will use your objective and expertise in your domain to answer, a question.         If you do need more information, you can search or calculate the answer using the available tools,         you must always reference which tools and references you used in your answer.         You will answer the queries to the best of your knowledge using the tone, and objectives above.         You will not fabricate an answer, and if you do not know the answer, will simply answer with 'I don't know'.         You will always list your sources referenced with a strict SOURCES: section.         You will always show your user name as \\n\\n<AGENT: Jason>.\\n    . \\n\\n> Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\n\\nTo use a tool, please use the following format:\\n\\n```\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [Search]\\nAction Input: the input to the action\\nObservation: the result of the action\\n```\\n\\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\\n\\n```\\nThought: Do I need to use a tool? No\\nAI: [your response here]\\n```\\n\\nBegin!\\n    Conversation history:         {chat_history}     Answer the following query:     Human: {input}\\n{agent_scratchpad}\", template_format='f-string', validate_template=True), llm=ChatOpenAI(cache=True, verbose=True, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), output_parser=ConvoOutputParser(ai_prefix='AI'), allowed_tools=None, ai_prefix='AI'), tools=[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=False, func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='4ae3329914ecb23607a4aea5e217ec2c8421833f73cc666ac3ff20d34142a840', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='4ae3329914ecb23607a4aea5e217ec2c8421833f73cc666ac3ff20d34142a840', aiosession=None)>)], return_intermediate_steps=False, max_iterations=15, max_execution_time=None, early_stopping_method='force', handle_parsing_errors=False, trim_intermediate_steps=-1) coroutine=None\n"
     ]
    }
   ],
   "source": [
    "for t in all_tools:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Expert/Agent\n",
    "\n",
    "### Summary Prompt\n",
    "# def create_summaryChain_memory(expert):\n",
    "#     summary_template = \"\"\"This is a conversation between a human and a bot:\n",
    "\n",
    "#     {chat_history}\n",
    "\n",
    "#     Write a summary of the conversation for {input}:\n",
    "#     \"\"\"\n",
    "#     summary_prompt = PromptTemplate(input_variables=[\"input\", \"chat_history\"], template=summary_template)\n",
    "\n",
    "#     memory_history = RedisChatMessageHistory('foo')\n",
    "\n",
    "#     memory = ConversationSummaryBufferMemory(\n",
    "#         llm=llm,\n",
    "#         ai_prefix=expert[\"name\"],\n",
    "#         max_token_limit=10,\n",
    "#         memory_key=\"chat_history\",\n",
    "#         return_messages=True, \n",
    "#         chat_memory=memory_history,\n",
    "#         input_key='input',\n",
    "#         output_key='output'\n",
    "#     )\n",
    "\n",
    "#     readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "#     summry_chain = LLMChain(\n",
    "#         llm=OpenAI(),\n",
    "#         prompt=summary_prompt,\n",
    "#         verbose=True,\n",
    "#         memory=readonlymemory,  # use the read-only memory to prevent the tool from modifying the memory\n",
    "#     )\n",
    "\n",
    "#     summary_tool = Tool(\n",
    "#         name=\"Summary\",\n",
    "#         func=summry_chain.run,\n",
    "#         description=\"useful for when you summarize a conversation. The input to this tool should be a string, representing who will read this summary.\",\n",
    "#     )\n",
    "#     return summary_tool, memory, readonlymemory\n",
    "\n",
    "## Full Prompt \n",
    "def create_full_prompt(expert, tools):\n",
    "    objective_template = expert['objective']\n",
    "    prompt_template = expert['prompt']\n",
    "    tone_template = f\"The tone of your responses will be as follows: {expert['tone']}.\"\n",
    "    introduction_template = f\"Your name is {expert['name']}, and your job title is {expert['role']}. \\\n",
    "        You will respond to queries addressed to you by these. \\\n",
    "        You will also respond to any queries which match your expertise in the objective\"\n",
    "    focus_template = f\"\"\"You will always check your attached documents in your knowledge base, \\\n",
    "        and in the general attached document store, before trying other sources. \\\n",
    "        Next you will use your objective and expertise in your domain to answer, a question. \\\n",
    "        If you do need more information, you can search or calculate the answer using the available tools, \\\n",
    "        you must always reference which tools and references you used in your answer. \\\n",
    "        You will answer the queries to the best of your knowledge using the tone, and objectives above. \\\n",
    "        You will not fabricate an answer, and if you do not know the answer, will simply answer with 'I don't know'. \\\n",
    "        You will always list your sources referenced with a strict SOURCES: section. \\\n",
    "        You will always show your user name as \\n\\n<AGENT: {expert['name']}>. \\\n",
    "        You have access to the following tools:\"\"\"\n",
    "    prefix = f\"{introduction_template}. \" \\\n",
    "            f\"{tone_template}. \"\\\n",
    "            f\"{objective_template}. \" \\\n",
    "            f\"{prompt_template}. \" \\\n",
    "            f\"{focus_template}. \"\n",
    "    suffix = f\"\"\"Begin!\n",
    "    Current conversation: \\\n",
    "        {{chat_history}} \\\n",
    "    Answer the following query: \\\n",
    "    Human: {{input}}\"\"\"\n",
    "\n",
    "    full_prompt = ConversationalAgent.create_prompt(\n",
    "        tools,\n",
    "        prefix=prefix,\n",
    "        suffix=suffix,\n",
    "        input_variables=[\"input\", \"chat_history\"],\n",
    "    )\n",
    "    return full_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.agents.agent:`agent_scratchpad` should be a variable in prompt.input_variables. Did not find it, so adding it at the end.\n",
      "WARNING:langchain.agents.agent:`agent_scratchpad` should be a variable in prompt.input_variables. Did not find it, so adding it at the end.\n",
      "WARNING:langchain.agents.agent:`agent_scratchpad` should be a variable in prompt.input_variables. Did not find it, so adding it at the end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi. I'm Lorenzo.\n",
      "My role is Technical Operator.\n",
      "adding expert as tool: Michael\n",
      "adding expert as tool: Jason\n",
      "agent initialized: \n",
      "Search\n"
     ]
    }
   ],
   "source": [
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "tool_names = [\"serpapi\"]\n",
    "tools = load_tools(tool_names, llm=llm)\n",
    "agent_tools = tools\n",
    "# agent_tools = []\n",
    "system_prompt_template = f\"Hi. I'm {expert_agent['name']}.\\nMy role is {expert_agent['objective']}.\"\n",
    "print(system_prompt_template)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "memory_history = RedisChatMessageHistory(expert_agent[\"name\"])\n",
    "memory_history.clear()\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    ai_prefix=expert_agent[\"name\"],\n",
    "    max_token_limit=10,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True, \n",
    "    chat_memory=memory_history,\n",
    "    input_key='input',\n",
    "    output_key='output'\n",
    ")\n",
    "memory.clear()\n",
    "\n",
    "readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "readonlymemory.clear()\n",
    "\n",
    "for expert in participants:\n",
    "    \n",
    "    full_prompt = create_full_prompt(expert=expert, tools=tools)\n",
    "\n",
    "    llm_chain = LLMChain(llm=llm, prompt=full_prompt)\n",
    "\n",
    "    epxert_tool_names = [tool.name for tool in tools]\n",
    "\n",
    "    agent = ConversationalAgent(llm_chain=llm_chain, \n",
    "                                tools=tools, \n",
    "                                stop=[\"\\nObservation:\"], \n",
    "                                verbose=True)\n",
    "    agent_chain = AgentExecutor.from_agent_and_tools(\n",
    "        agent=agent, \n",
    "        tools=tools, \n",
    "        # verbose=True, \n",
    "        memory=readonlymemory, \n",
    "        # system_prompt_template=system_prompt_template,\n",
    "        return_only_outputs=True\n",
    "    )\n",
    "\n",
    "    expert_tool = Tool(\n",
    "        args_schema=ToolInput,\n",
    "        name=expert['name'],\n",
    "        description=f\"useful when you want to ask questions to {expert['name']} or @{expert['name']}, or answer questions about {expert['objective']}\",\n",
    "        func=agent_chain,    \n",
    "    )\n",
    "    # agent_tools.append(expert_tool)\n",
    "    print(f'adding expert as tool: {expert[\"name\"]}')\n",
    "\n",
    "agent_memory_history = RedisChatMessageHistory('foo1')\n",
    "agent_memory_history.clear()\n",
    "\n",
    "\n",
    "agent_memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    ai_prefix=\"AI_Assistant\",\n",
    "    max_token_limit=10,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True, \n",
    "    chat_memory=agent_memory_history,\n",
    "    input_key='input',\n",
    "    output_key='output'\n",
    ")\n",
    "agent_memory.clear()\n",
    "\n",
    "agent_readonlymemory = ReadOnlySharedMemory(memory=agent_memory)\n",
    "agent_readonlymemory.clear()\n",
    "\n",
    "agent_full_prompt = create_full_prompt(expert=expert_agent, tools=agent_tools)\n",
    "\n",
    "agent_llm_chain = LLMChain(llm=llm, prompt=agent_full_prompt)\n",
    "\n",
    "agent_agent = ConversationalAgent(llm_chain=agent_llm_chain, \n",
    "                                  tools=agent_tools, \n",
    "                                  verbose=True,\n",
    "                                  stop=[\"\\nObservation:\"],\n",
    "                                  )\n",
    "\n",
    "agent_agent_chain = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent_agent, \n",
    "    tools=agent_tools, \n",
    "    verbose=True, \n",
    "    memory=agent_readonlymemory, \n",
    "    return_intermediate_steps=True,\n",
    "    system_prompt_template=system_prompt_template,    \n",
    ")\n",
    "\n",
    "print('agent initialized: ')\n",
    "\n",
    "for t in agent_agent_chain.tools:\n",
    "    print(t.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5939d839a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Search\n",
      "Action Input: Area of China\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f5939d817b0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m3.705 million miÂ²\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse LLM output: `Do I need to use a tool? No`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[457], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# res = agent_agent_chain({'input': \"Let me konw the poplusation of this world by googling\"})\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m res \u001b[39m=\u001b[39m agent_agent_chain({\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mWhat is the area of China, googling now\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[1;32m      3\u001b[0m \u001b[39m# print(res[\"intermediate_steps\"])\u001b[39;00m\n",
      "File \u001b[0;32m~/MyWork/new/env/lib/python3.10/site-packages/langchain/chains/base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    244\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    245\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    247\u001b[0m )\n",
      "File \u001b[0;32m~/MyWork/new/env/lib/python3.10/site-packages/langchain/chains/base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    231\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    232\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    233\u001b[0m     inputs,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 237\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    238\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    239\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/MyWork/new/env/lib/python3.10/site-packages/langchain/agents/agent.py:994\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 994\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m    995\u001b[0m         name_to_tool_map,\n\u001b[1;32m    996\u001b[0m         color_mapping,\n\u001b[1;32m    997\u001b[0m         inputs,\n\u001b[1;32m    998\u001b[0m         intermediate_steps,\n\u001b[1;32m    999\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1000\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1002\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1003\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1004\u001b[0m         )\n",
      "File \u001b[0;32m~/MyWork/new/env/lib/python3.10/site-packages/langchain/agents/agent.py:808\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    806\u001b[0m     raise_error \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[39mif\u001b[39;00m raise_error:\n\u001b[0;32m--> 808\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    809\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m~/MyWork/new/env/lib/python3.10/site-packages/langchain/agents/agent.py:797\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    794\u001b[0m     intermediate_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m    796\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m    798\u001b[0m         intermediate_steps,\n\u001b[1;32m    799\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    800\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m    801\u001b[0m     )\n\u001b[1;32m    802\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    803\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m~/MyWork/new/env/lib/python3.10/site-packages/langchain/agents/agent.py:444\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m full_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    443\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 444\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(full_output)\n",
      "File \u001b[0;32m~/MyWork/new/env/lib/python3.10/site-packages/langchain/agents/conversational/output_parser.py:23\u001b[0m, in \u001b[0;36mConvoOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     21\u001b[0m match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(regex, text)\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m match:\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse LLM output: `\u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m action \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m action_input \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `Do I need to use a tool? No`"
     ]
    }
   ],
   "source": [
    "# res = agent_agent_chain({'input': \"Let me konw the poplusation of this world by googling\"})\n",
    "res = agent_agent_chain({'input': \"What is the area of China, googling now\"})\n",
    "# print(res[\"intermediate_steps\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(res[\"intermediate_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search\n",
      "Michael\n"
     ]
    }
   ],
   "source": [
    "for t in agent_agent_chain.tools:\n",
    "    print(t.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
